# 线性回归

预测连续值

## 方法

1. 正规方程

   $(X^{T}X)^{-1}X^{T}Y​$

2. 梯度下降

```python
import numpy as np

def loadDataSet(filename):
    numFeat = 0
    dataArr = []; labelArr = []
    with open(filename) as f:
        numFeat = len(f.readline().split('\t')) - 1
        for line in f.readlines():
            lineArr = []
            currLine = line.strip().split('\t')
            for i in range(numFeat):
                lineArr.append(float(currLine[i]))
            dataArr.append(lineArr)
            labelArr.append(float(currLine[-1]))
    return dataArr, labelArr
# 正规方程
def standRegres(xArr, yArr):
    xMat = np.mat(xArr); yMat = np.mat(yArr).T
    xTx = xMat.T * xMat
    if np.linalg.det(xTx) == 0.0:
        print('This matrix is singular')
        return
    ws = xTx.I * (xMat.T * yMat)
    return ws
# 梯度下降
def granDescent(dataArr, labelArr, iterNum=200):
    dataMat = np.mat(dataArr); labelMat = np.mat(labelArr).T
    m,n = np.shape(dataMat)
    weights = np.ones((n,1))
    alpha = 0.001 # 步长为0.01时，就会导致无法找到最小值
    for j in range(iterNum):
        h = dataMat * weights
        error = h - labelMat
        weights = weights - alpha*dataMat.T*error
    return weights

def plot(dataArr, labelArr, weights):
    import matplotlib.pyplot as plt
    xArr = np.array(dataArr).T[1] # 取每个样本的第二个特征
    yArr = np.array(labelArr)
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(xArr, yArr)
    # 将点按序排列，否则绘图会出现问题
    xCopy = dataArr.copy()
    xCopy.sort()
    # 由于x0=0，故h=w[0]+w[1]*x1，故x1作为x轴
    h = np.array(xCopy) * weights # 计算预测值
    ax.plot(np.array(xCopy).T[1], h)
    plt.show()
```

## 正则化

### 梯度下降

$$
J(\theta)=\frac{1}{2m} [\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})^{2} + \lambda \sum_{j=1}^{n} \theta_j^{2}]
$$

则

$\theta_0:=\theta_0-\frac{\alpha}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_0$

$\theta_j:=\theta_j(1-\frac{\alpha\lambda}{m})-\frac{\alpha}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$

### 正规方程

$$
\theta=(X^{T}X + \lambda \begin{bmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}) X^{T} Y
$$

## 局部加权线性回归

线性回归可能出现欠拟合问题（不能取得最好的预测效果），因为它求的是具有最小均方误差的无偏估计。局部加权线性回归是**引入一些偏差从而降低预测的均方误差**的一种方法。

