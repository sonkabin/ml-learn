# 线性代数

## SVD

### 是什么

矩阵分解技术

### 为什么是

将原始矩阵Data分解成3个矩阵$U、 \sum、 V^T$，若原始矩阵是m行n列，则$U、 \sum、 V^T$分别是m行m列，m行n列，n行n列矩阵。其中$\sum$是对角矩阵，且元素从大到小排列，这些元素被称为奇异值。这里的奇异值是$Data * Data^T$特征值的平方根（因此从某个元素之后，其余奇异值都为0）

**其中，U和V都是酉矩阵，即$U^T U=E$**

### 步骤

观察$\sum$，将近似为0的值去掉，原始数据集可用近似结果表示（如：$Data_{7*5} = U_{7*7} \sum_{7*5} V^T_{5*5}$，去掉2个近似为0的值后，$Data_{7*5} \approx U_{7*3} \sum_{3*3} V^T_{3*5}$）

### 应用

1. PCA

   降维：$z = U_{reduce}^T x$ 。吴的视频中。		机器学习实战中，降维为$z = (U_{reduce} \times \sum_{reduce})^T  x$ 

   还原：$X_{approx} = U_{reduce}z$

   **可参考西瓜书P231**

2. 推荐系统

### 建议

1. 保留几个奇异值？

   一个典型的做法是保留90%的能量

## 齐次方程

Ax=0有非零解的充要条件是r(A)<n，只有零解的充要条件是r(A)=n

### 基础解系求法

例：求如下矩阵的基础解系
$$
\left(
\begin{matrix}
4 & -1 & -1 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{matrix}
\right)
$$
令x1=1,x3=0，得基础解系$(1,4,0)^T$

令x1=0,x3=1，得基础解系$(0,-1,1)^T$



## 矩阵求导

前提：分母布局，即$\vec x=(x_1, x_2, ..., x_n)^T$

### 定义

- 设y=f(x)是$ x$的标量函数，其中$ x=(x_1, x_2, ..., x_n)^T$, 则f(x)对$x$的梯度为$\frac {\partial y} {\partial x} = \left[  \begin{matrix}  \frac {\partial y} {\partial x_1}  \\  \frac {\partial y} {\partial x_2}  \\ \vdots  \\  \frac {\partial y} {\partial x_n}  \end{matrix}  \right]$
- 标量对矩阵求导，$\frac {\partial y} {\partial X}  = \left[
  \begin{matrix}
  \frac {\partial y} {\partial x_{11}} & \frac {\partial y} {\partial x_{12}} & \cdots & \frac {\partial y} {\partial x_{1n}} \\
  \frac {\partial y} {\partial x_{21}} & \frac {\partial y} {\partial x_{22}} & \cdots & \frac {\partial y} {\partial x_{2n}} \\  \vdots  &  \vdots  &  \ddots  &  \vdots   \\
  \frac {\partial y} {\partial x_{m1}} & \frac {\partial y} {\partial x_{m2}} & \cdots & \frac {\partial y} {\partial x_{mn}}
  \end{matrix}
  \right] $

### 参考

[知乎：矩阵求导术（上）](https://zhuanlan.zhihu.com/p/24709748)

